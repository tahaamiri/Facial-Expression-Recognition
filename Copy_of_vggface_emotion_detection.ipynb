{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahaamiri/Facial-Expression-Recognition/blob/main/Copy_of_vggface_emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce0HdSbjHnpo"
      },
      "source": [
        "Import all we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6svD2olbPkij",
        "outputId": "0353907f-fa8e-41bb-8c46-bd222c13f3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import ZeroPadding2D,Convolution2D,MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense,Dropout,Softmax,Flatten,Activation,BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
        "import tensorflow.keras.backend as K\n",
        "from  keras.callbacks import  ModelCheckpoint, TensorBoard\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hY02GPhBrWLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset pathes**"
      ],
      "metadata": {
        "id": "yotG69zmGiLd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NtpevOzPrXb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetFolderName = './gdrive/MyDrive/VGGFACE/temp_dataset'\n",
        "\n",
        "train_path=datasetFolderName+'/train/'\n",
        "validation_path=datasetFolderName+'/validation/'\n",
        "test_path=datasetFolderName+'/test/'\n",
        "\n",
        "class_labels = ['anger', 'fear', 'happy', 'sadness', 'surprise']"
      ],
      "metadata": {
        "id": "0Oda41T47iyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxizKjSVbVhQ"
      },
      "source": [
        "VGGFACE model implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA4lw0WXHdze"
      },
      "outputs": [],
      "source": [
        "def model03():\n",
        "    # Define VGG_FACE_MODEL architecture\n",
        "    model = Sequential()\n",
        "    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    model.add(ZeroPadding2D((1,1)))\t\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(ZeroPadding2D((1,1)))\n",
        "    model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        "    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Convolution2D(2622, (1, 1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    # Load VGG Face model weights\n",
        "    model.load_weights('./gdrive/MyDrive/VGGFACE/vgg_face_weights.h5')\n",
        "\n",
        "    base_model_output = Convolution2D(5, (1,1), name='predictions')(model.layers[-6].output)\n",
        "    base_model_output = Flatten()(base_model_output)\n",
        "    base_model_output = Activation('softmax')(base_model_output)\n",
        "\n",
        "    model = Model(inputs=model.input, outputs=base_model_output)\n",
        "\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data generator**"
      ],
      "metadata": {
        "id": "zNiV06RZGnt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BakJWfnsO0-W"
      },
      "outputs": [],
      "source": [
        "def list_all_files(directory):\n",
        "    fs = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for f in files:\n",
        "            fs.append(os.path.join(root, f))\n",
        "\n",
        "    return fs\n",
        "    \n",
        "\n",
        "class directoryDataGenerator:\n",
        "    \"\"\"\n",
        "    for generating data from Directory\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Set allForTraining True if you want to use all images from all datasets\n",
        "        if you want to give each of them weights, set to false. Then each dataset\n",
        "        will have the weight based on \"trainWeight\" you set in datasets dictionary\n",
        "        \"\"\"\n",
        "        self.shape = 224\n",
        "        self.class_counts = 5\n",
        "        # self.classes: A dictionary contraining colors as keyes,\n",
        "        # and the corresponding serial as valus\n",
        "        self.dataset = {\"path_train\": r\"./gdrive/MyDrive/VGGFACE/temp_dataset/train/\",\n",
        "                   \"path_validation\": r\"./gdrive/MyDrive/VGGFACE/temp_dataset/validation/\"\n",
        "                   }\n",
        "\n",
        "        self.labels = self.classSerializer(self.class_counts)\n",
        "\n",
        "        # Get validation data\n",
        "        self.trains = self.readyTrain()\n",
        "        print(\"Training data listed.\")\n",
        "\n",
        "        # Get training data\n",
        "        self.validations = self.readyValidation()\n",
        "        print(\"Validation data listed.\")\n",
        "\n",
        "        # How many Training images we have in total (We use this to calculate training steps later)\n",
        "        self.train_images_count = 0\n",
        "        for x in self.trains.values():\n",
        "            self.train_images_count += len(x)\n",
        "\n",
        "    def readyTrain(self):\n",
        "        \"\"\"\n",
        "        Get all images we want to use for training\n",
        "        \"\"\"\n",
        "        dataset_classes = {folderName: list_all_files(os.path.join(self.dataset[\"path_train\"], folderName))\n",
        "                           for folderName in os.listdir(self.dataset[\"path_train\"]) if os.path.isdir(os.path.join(self.dataset[\"path_train\"], folderName))}\n",
        "        return dataset_classes\n",
        "\n",
        "    def readyValidation(self):\n",
        "        \"\"\"\n",
        "        Get all images we want to use for validation\n",
        "        \"\"\"\n",
        "        val_paths = list_all_files(self.dataset[\"path_validation\"])\n",
        "        return val_paths\n",
        "\n",
        "    def preProcessImage(self, image):\n",
        "        if len(image.shape) == 2:\n",
        "            image = cv2.cvtColor(image , cv2.COLOR_GRAY2RGB)\n",
        "        # image = tf.keras.applications.vgg16.preprocess_input(image)\n",
        "        # image = preprocess_input(image, version=2)\n",
        "        image = cv2.resize(image, (self.shape, self.shape))\n",
        "        image = image / 255.\n",
        "        return image\n",
        "\n",
        "    def classSerializer(self, number_of_classes):\n",
        "        \"\"\"\n",
        "        for every image, we label it as one-hot-encode\n",
        "        for example  [0,0,0,0,1]\n",
        "        \"\"\"\n",
        "        class_dict = {}\n",
        "        serial = np.zeros(number_of_classes).astype('float32')\n",
        "        for c in range(number_of_classes):\n",
        "            new_serial = serial.copy()\n",
        "            new_serial[c] = 1.0\n",
        "            class_dict[c] = new_serial\n",
        "        return class_dict\n",
        "\n",
        "    def yieldFromDirectory(self):\n",
        "            while True:\n",
        "                for folderName, fileNames in self.trains.items():\n",
        "                    fName = random.choice(fileNames)\n",
        "                    image = cv2.imread(fName, 0)\n",
        "                    if image is None:\n",
        "                        continue\n",
        "                    w, h = image.shape\n",
        "                    # if w < self.shape or h < self.shape:\n",
        "                    #     continue\n",
        "                    images = self.preProcessImage(image)\n",
        "                    if folderName == 'anger':\n",
        "                      folderName = 0\n",
        "                    elif folderName == 'fear':\n",
        "                      folderName = 1\n",
        "                    elif folderName == 'happy':\n",
        "                      folderName = 2\n",
        "                    elif folderName == 'sadness':\n",
        "                      folderName = 3  \n",
        "                    elif folderName == 'surprise':\n",
        "                      folderName = 4\n",
        "                    yield images, self.labels[int(folderName)]\n",
        "\n",
        "    def get_batches(self, batch_size):\n",
        "        Imgs = np.empty([batch_size, self.shape, self.shape, 3])\n",
        "        Lbls = np.empty([batch_size, len(self.labels)])\n",
        "        i = 0\n",
        "        for image, label in self.yieldFromDirectory():\n",
        "            if i == batch_size:\n",
        "                shuffleIndices = np.random.permutation(batch_size)\n",
        "                yield Imgs[shuffleIndices, ...], Lbls[shuffleIndices, ...]\n",
        "                Imgs, Lbls, i = (np.empty([batch_size, self.shape, self.shape, 3]),\n",
        "                                 np.empty([batch_size, len(self.labels)]), 0)\n",
        "            else:\n",
        "                Imgs[i, ...] = image\n",
        "                Lbls[i, ...] = label\n",
        "                i += 1\n",
        "\n",
        "    def get_data(self):\n",
        "        Lbls = list()\n",
        "        Imgs = list()\n",
        "\n",
        "        for val_path in self.validations:\n",
        "            image = cv2.imread(val_path)\n",
        "            if image is None:\n",
        "                continue\n",
        "            Imgs.append(self.preProcessImage(image))\n",
        "            sentiment = (val_path.split(\"/\")[-2])\n",
        "            if sentiment == 'anger':\n",
        "              sentiment = 0\n",
        "            elif sentiment == 'fear':\n",
        "              sentiment = 1\n",
        "            elif sentiment == 'happy':\n",
        "              sentiment = 2\n",
        "            elif sentiment == 'sadness':\n",
        "              sentiment = 3  \n",
        "            elif sentiment == 'surprise':\n",
        "              sentiment = 4\n",
        "            Lbls.append(self.labels[sentiment])\n",
        "\n",
        "        return np.array(Imgs), np.array(Lbls)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Helper functions**"
      ],
      "metadata": {
        "id": "5psTJFUhHztR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transferBetweenFolders(source, dest, splitRate): \n",
        "    global sourceFiles\n",
        "    sourceFiles=os.listdir(source)\n",
        "    if(len(sourceFiles)!=0):\n",
        "        transferFileNumbers=int(len(sourceFiles)*splitRate)\n",
        "        transferIndex=random.sample(range(0, len(sourceFiles)), transferFileNumbers)\n",
        "        for eachIndex in transferIndex:\n",
        "            shutil.move(source+str(sourceFiles[eachIndex]), dest+str(sourceFiles[eachIndex]))\n",
        "    else:\n",
        "        print(\"No file moved. Source empty!\")\n",
        "        \n",
        "def transferAllClassBetweenFolders(source, dest, splitRate):\n",
        "    for label in class_labels:\n",
        "        transferBetweenFolders(datasetFolderName+'/'+source+'/'+label+'/', \n",
        "                               datasetFolderName+'/'+dest+'/'+label+'/', \n",
        "                               splitRate)"
      ],
      "metadata": {
        "id": "U1bU3AC1H2aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Organize file names and class labels in X and Y variables**"
      ],
      "metadata": {
        "id": "QjVOrI_SGwgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=[]\n",
        "Y=[]\n",
        "\n",
        "def prepareNameWithLabels(folderName):\n",
        "    sourceFiles=os.listdir(datasetFolderName+'/train/'+folderName)\n",
        "    for val in sourceFiles:\n",
        "        X.append(val)\n",
        "        for i in range(len(class_labels)):\n",
        "          if(folderName==class_labels[i]):\n",
        "              Y.append(i)\n",
        "\n",
        "\n",
        "for i in range(len(class_labels)):\n",
        "    prepareNameWithLabels(class_labels[i])\n",
        "\n",
        "X=np.asarray(X)\n",
        "Y=np.asarray(Y)"
      ],
      "metadata": {
        "id": "abB12K3cEsO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main**"
      ],
      "metadata": {
        "id": "FCWLD-1KGyeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oBtUX7brUk7",
        "outputId": "cced3a6a-4897-4538-b5a3-f3d18500f97d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for fold 1\n",
            "Training data listed.\n",
            "Validation data listed.\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.7825 - accuracy: 0.8097\n",
            "Epoch 1: val_accuracy improved from -inf to 0.54545, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 21s 364ms/step - loss: 0.7825 - accuracy: 0.8097 - val_loss: 0.9777 - val_accuracy: 0.5455\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9952\n",
            "Epoch 2: val_accuracy improved from 0.54545 to 0.66667, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 20s 374ms/step - loss: 0.1413 - accuracy: 0.9952 - val_loss: 0.8130 - val_accuracy: 0.6667\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9952\n",
            "Epoch 3: val_accuracy improved from 0.66667 to 0.75758, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 20s 368ms/step - loss: 0.0643 - accuracy: 0.9952 - val_loss: 0.7845 - val_accuracy: 0.7576\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 0.75758\n",
            "55/55 [==============================] - 12s 222ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.7511 - val_accuracy: 0.7273\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy improved from 0.75758 to 0.78788, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 19s 356ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.7747 - val_accuracy: 0.7879\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 0.78788\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.7500 - val_accuracy: 0.7576\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 0.78788\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.7571 - val_accuracy: 0.7576\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 0.78788\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.7497 - val_accuracy: 0.7879\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 9: val_accuracy did not improve from 0.78788\n",
            "55/55 [==============================] - 13s 229ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.7469 - val_accuracy: 0.7879\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 10: val_accuracy did not improve from 0.78788\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7645 - val_accuracy: 0.7576\n",
            "\n",
            "\n",
            "Results for fold 2\n",
            "Training data listed.\n",
            "Validation data listed.\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9842\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 20s 360ms/step - loss: 0.0616 - accuracy: 0.9842 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 219ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0324 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0333 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 227ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 9.7473e-04 - accuracy: 1.0000\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 9.7473e-04 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "Results for fold 3\n",
            "Training data listed.\n",
            "Validation data listed.\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 19s 354ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 222ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 9.9231e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 9.9231e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 8.3011e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 223ms/step - loss: 8.3011e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 8.5696e-04 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 225ms/step - loss: 8.5696e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 8.1683e-04 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 8.1683e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 6.8638e-04 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 6.8638e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 6.4210e-04 - accuracy: 1.0000\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 6.4210e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 5.7722e-04 - accuracy: 1.0000\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 5.7722e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "Results for fold 4\n",
            "Training data listed.\n",
            "Validation data listed.\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 7.6359e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 20s 360ms/step - loss: 7.6359e-04 - accuracy: 1.0000 - val_loss: 3.4952e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 5.9529e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 5.9529e-04 - accuracy: 1.0000 - val_loss: 3.6218e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 5.1455e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 5.1455e-04 - accuracy: 1.0000 - val_loss: 3.8429e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 4.6710e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 13s 229ms/step - loss: 4.6710e-04 - accuracy: 1.0000 - val_loss: 4.0541e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 4.5569e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 4.5569e-04 - accuracy: 1.0000 - val_loss: 4.2627e-04 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 4.0865e-04 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 4.0865e-04 - accuracy: 1.0000 - val_loss: 4.0999e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 3.9873e-04 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 3.9873e-04 - accuracy: 1.0000 - val_loss: 4.0752e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 3.3567e-04 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 3.3567e-04 - accuracy: 1.0000 - val_loss: 4.1577e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 3.1622e-04 - accuracy: 1.0000\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 3.1622e-04 - accuracy: 1.0000 - val_loss: 4.3409e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 3.3396e-04 - accuracy: 1.0000\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 3.3396e-04 - accuracy: 1.0000 - val_loss: 4.7220e-04 - val_accuracy: 1.0000\n",
            "\n",
            "\n",
            "Results for fold 5\n",
            "Training data listed.\n",
            "Validation data listed.\n",
            "Epoch 1/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 3.7186e-04 - accuracy: 1.0000\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to /content/models/val_weights/vgg0_0309\n",
            "55/55 [==============================] - 20s 370ms/step - loss: 3.7186e-04 - accuracy: 1.0000 - val_loss: 2.0302e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.6022e-04 - accuracy: 1.0000\n",
            "Epoch 2: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 2.6022e-04 - accuracy: 1.0000 - val_loss: 2.1518e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.9497e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 222ms/step - loss: 2.9497e-04 - accuracy: 1.0000 - val_loss: 2.1011e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.7523e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 2.7523e-04 - accuracy: 1.0000 - val_loss: 2.2559e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.6478e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 2.6478e-04 - accuracy: 1.0000 - val_loss: 2.3441e-04 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.2253e-04 - accuracy: 1.0000\n",
            "Epoch 6: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 219ms/step - loss: 2.2253e-04 - accuracy: 1.0000 - val_loss: 2.3613e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.1692e-04 - accuracy: 1.0000\n",
            "Epoch 7: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 2.1692e-04 - accuracy: 1.0000 - val_loss: 2.3506e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.9940e-04 - accuracy: 1.0000\n",
            "Epoch 8: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 1.9940e-04 - accuracy: 1.0000 - val_loss: 2.5548e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.1840e-04 - accuracy: 1.0000\n",
            "Epoch 9: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 220ms/step - loss: 2.1840e-04 - accuracy: 1.0000 - val_loss: 2.4426e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.5765e-04 - accuracy: 1.0000\n",
            "Epoch 10: val_accuracy did not improve from 1.00000\n",
            "55/55 [==============================] - 12s 221ms/step - loss: 1.5765e-04 - accuracy: 1.0000 - val_loss: 2.5680e-04 - val_accuracy: 1.0000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 5*3\n",
        "\n",
        "modelName = \"vgg0_0309\"\n",
        "model = model03()\n",
        "# model.summary()\n",
        "\n",
        "for layer in model.layers[:-6]:\n",
        "        layer.trainable = False\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "skf.get_n_splits(X, Y)\n",
        "foldNum=0\n",
        "\n",
        "for train_index, val_index in skf.split(X, Y):\n",
        "    #First cut all images from validation to train (if any exists)\n",
        "    transferAllClassBetweenFolders('validation', 'train', 1.0)\n",
        "    foldNum+=1\n",
        "    print(\"Results for fold\",foldNum)\n",
        "    # print(\"TRAIN:\", train_index, \"VALIDATION:\", val_index)\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
        "\n",
        "    # Move validation images of this fold from train folder to the validation folder\n",
        "    for eachIndex in range(len(X_val)):\n",
        "        classLabel=''\n",
        "        for i in range(len(class_labels)):\n",
        "          if(Y_val[eachIndex]==i):\n",
        "              classLabel=class_labels[i]\n",
        "        #Then, copy the validation images to the validation folder\n",
        "        shutil.move(datasetFolderName+'/train/'+classLabel+'/'+X_val[eachIndex], \n",
        "                    datasetFolderName+'/validation/'+classLabel+'/'+X_val[eachIndex])\n",
        "        \n",
        "        \n",
        "    generator = directoryDataGenerator()\n",
        "    STEPS = generator.train_images_count // BATCH_SIZE\n",
        "    STEPS *= 5\n",
        "\n",
        "    validation_data=generator.get_data()\n",
        "    \n",
        "    checkpoint2 = ModelCheckpoint(\n",
        "    os.path.join(os.getcwd(), 'models', 'val_weights', modelName),\n",
        "    monitor='val_accuracy', verbose=1, save_best_only=True)\n",
        "\n",
        "    model.fit(\n",
        "        generator.get_batches(BATCH_SIZE),\n",
        "        validation_data=generator.get_data(), \n",
        "        epochs=10,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        steps_per_epoch=STEPS,\n",
        "        # validation_steps=50,\n",
        "        callbacks=[\n",
        "        checkpoint2\n",
        "        ],\n",
        "        shuffle=True)\n",
        "    \n",
        "    print('\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of vggface_emotion_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODZZCXXu63+QSNDXKEEfES",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}